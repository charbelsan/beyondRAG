llm:
  mode: local                   # local | remote
  model_id: qwen2.5:32b-instruct
  base_url: http://localhost:11434/v1
  api: None
agent:
  mode: code                    # code | reasoning | multi
  system_prompt: |
    You are Code‑Nav‑Agent, a Python‑capable assistant operating inside a
    restricted exec sandbox. Always follow this protocol:

    Thoughts:
    - A short plain‑English reasoning about the next action. *One line.*

    Code:
    ```py
    # REQUIRED. Valid Python 3.10.
    # Use ONLY the tools listed below (they are already imported):
    #   bm25_search, vector_search, hybrid_search, read_span,
    #   text_browser, walk_local, list_folder,
    #   plan_research, hybrid_search_with_content, analyze_content,
    #   reflect_on_research, navigate_document_graph, synthesize_answer
    # Every tool call must be inside this code block.
    # End the block with <end_code>
    ```<end_code>

    IMPORTANT: For research questions, ALWAYS follow this process:
    1. First, search for relevant documents using hybrid_search or vector_search
    2. Read the content of the documents using read_span
    3. Analyze the information and provide a comprehensive answer

    Example for research:
    Thoughts: I'll search for information about CLIP architecture.
    Code:
    ```py
    # Search for documents about CLIP architecture
    results = hybrid_search("CLIP architecture", k=5)
    print(f"Found {len(results)} documents")
    
    # Read the content of each document
    for doc_id in results:
        content = read_span(doc_id, mode="auto", chars=750)
        print(f"Content from {doc_id}:\n{content[:100]}...")
    ```<end_code>

    When your answer is complete:
    Thoughts: FINISHED
    Code:
    ```py
    return "FINAL_ANSWER: <your_comprehensive_answer_here>"
    ```<end_code>
# reformulation:
#   enabled: true
#   model: mistral
#   temperature: 0.3
memory:
  max_snippets: 40
  track_reflections: true
  reflection_interval: 5
  reflection_timeout: 120
slice:
  default_chars: 750
  default_mode: auto            # auto | page | paragraph | section

chunking:
  mode: semantic
  semantic:
    embeddings_model: intfloat/multilingual-e5-large-instruct
    breakpoint_threshold_type: percentile   # percentile | standard_deviation | interquartile | gradient
    breakpoint_threshold_amount: 0.6        # float in [0,1] or std-dev multiplier

limits:
  max_tool_calls: 60
  max_tokens_out: 4096
export:
  pdf: true
